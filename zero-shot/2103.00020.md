# Learning Transferable Visual Models From Natural Language Supervision

## Introduction and Motivation
- Could a scalable pre-training method which learn directly
from web text surpass the pre-training on high-quality crowd-labeled datasets ?
    - Learning image representation from raw text supervision for pre-training task. 
    - Task : Caption prediction using the (image, text) pairs collected from the internet.
    - Enable zero-shot transfer of the model to downstream tasks.

- Code and weights can be accessed from [LINK](https://github.com/OpenAI/CLIP)

- Proposed Method : CLIP (Contrastive Language-Image Pre-training) is an efficient method of learning image representations from natural language supervision.

## Reference
1. [Radford, Alec, et al. "Learning transferable visual models from natural language supervision." International Conference on Machine Learning. PMLR, 2021.](https://arxiv.org/abs/2103.00020)
